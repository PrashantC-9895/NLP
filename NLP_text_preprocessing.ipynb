{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "tvTBBZxBqy2P"
      },
      "outputs": [],
      "source": [
        "#NLP we need 2 frameworks NLTK and SPACY\n",
        "\n",
        "#NLTK --> Natural Language Toolkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#we need to load the data and preprocess the data then develop the NLP model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ78drO4rpNQ",
        "outputId": "2cd6e6df-9f41-43dd-deb8-a6f8fbd6d9c9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collect the data"
      ],
      "metadata": {
        "id": "4LjWQPQ6vX2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Manually analyzing hundreds of unstructured text-based data sources is tedious and time-consuming. What's worse is that since it's in text format, you know this process can be made easier, but you're unsure how to do it. If you’re in a data-driven company that relies on such data sources to make critical decisions to optimize and improve your customer experience processes, keep reading.And he's right. Too often, we get stuck in the DRIP syndrome where we're happily generating data—but can't generate meaningful insights from them. Also, when you consider that 80% to 90% of data is unstructured,  there’s so much potential waiting to be unlocked.In this article, we’ll discuss the concept of textual data and how you can use it to extract valuable insights for your customer support operations.\"\"\"\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijmXpCk6rpQ2",
        "outputId": "92dcc82a-994f-479e-8765-07b5b3f2ddac"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manually analyzing hundreds of unstructured text-based data sources is tedious and time-consuming. What's worse is that since it's in text format, you know this process can be made easier, but you're unsure how to do it. If you’re in a data-driven company that relies on such data sources to make critical decisions to optimize and improve your customer experience processes, keep reading.And he's right. Too often, we get stuck in the DRIP syndrome where we're happily generating data—but can't generate meaningful insights from them. Also, when you consider that 80% to 90% of data is unstructured,  there’s so much potential waiting to be unlocked.In this article, we’ll discuss the concept of textual data and how you can use it to extract valuable insights for your customer support operations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization = it means understanding the sentences and words from given data"
      ],
      "metadata": {
        "id": "7xfCgXpix1Qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Sentence Tokenization\n",
        "from nltk.tokenize import sent_tokenize  #sentence tokenization\n",
        "\n",
        "total_sentence = sent_tokenize(text)\n",
        "\n",
        "print(total_sentence)\n",
        "\n",
        "print(f'total number of sentences in data : {len(total_sentence)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi08lLhLrpT2",
        "outputId": "4504a40f-3fbc-4188-fcae-c5efc4525aa7"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Manually analyzing hundreds of unstructured text-based data sources is tedious and time-consuming.', \"What's worse is that since it's in text format, you know this process can be made easier, but you're unsure how to do it.\", \"If you’re in a data-driven company that relies on such data sources to make critical decisions to optimize and improve your customer experience processes, keep reading.And he's right.\", \"Too often, we get stuck in the DRIP syndrome where we're happily generating data—but can't generate meaningful insights from them.\", 'Also, when you consider that 80% to 90% of data is unstructured,  there’s so much potential waiting to be unlocked.In this article, we’ll discuss the concept of textual data and how you can use it to extract valuable insights for your customer support operations.']\n",
            "total number of sentences in data : 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word tokenize\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "total_word = word_tokenize(text)\n",
        "\n",
        "print(total_word)\n",
        "\n",
        "print(f'total words in data : {len(total_word)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqQZCdtDrpXI",
        "outputId": "f56e0a50-a499-4ed8-f73e-a8484500d17f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Manually', 'analyzing', 'hundreds', 'of', 'unstructured', 'text-based', 'data', 'sources', 'is', 'tedious', 'and', 'time-consuming', '.', 'What', \"'s\", 'worse', 'is', 'that', 'since', 'it', \"'s\", 'in', 'text', 'format', ',', 'you', 'know', 'this', 'process', 'can', 'be', 'made', 'easier', ',', 'but', 'you', \"'re\", 'unsure', 'how', 'to', 'do', 'it', '.', 'If', 'you', '’', 're', 'in', 'a', 'data-driven', 'company', 'that', 'relies', 'on', 'such', 'data', 'sources', 'to', 'make', 'critical', 'decisions', 'to', 'optimize', 'and', 'improve', 'your', 'customer', 'experience', 'processes', ',', 'keep', 'reading.And', 'he', \"'s\", 'right', '.', 'Too', 'often', ',', 'we', 'get', 'stuck', 'in', 'the', 'DRIP', 'syndrome', 'where', 'we', \"'re\", 'happily', 'generating', 'data—but', 'ca', \"n't\", 'generate', 'meaningful', 'insights', 'from', 'them', '.', 'Also', ',', 'when', 'you', 'consider', 'that', '80', '%', 'to', '90', '%', 'of', 'data', 'is', 'unstructured', ',', 'there', '’', 's', 'so', 'much', 'potential', 'waiting', 'to', 'be', 'unlocked.In', 'this', 'article', ',', 'we', '’', 'll', 'discuss', 'the', 'concept', 'of', 'textual', 'data', 'and', 'how', 'you', 'can', 'use', 'it', 'to', 'extract', 'valuable', 'insights', 'for', 'your', 'customer', 'support', 'operations', '.']\n",
            "total words in data : 154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove unwanted stuff : - punctutations\n",
        "\n",
        "text #to develop NPL application we need keep only words other than everything should be removed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "MmkYr9BqrpbP",
        "outputId": "c71b3eec-4454-4e12-959b-74951f427372"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Manually analyzing hundreds of unstructured text-based data sources is tedious and time-consuming. What's worse is that since it's in text format, you know this process can be made easier, but you're unsure how to do it. If you’re in a data-driven company that relies on such data sources to make critical decisions to optimize and improve your customer experience processes, keep reading.And he's right. Too often, we get stuck in the DRIP syndrome where we're happily generating data—but can't generate meaningful insights from them. Also, when you consider that 80% to 90% of data is unstructured,  there’s so much potential waiting to be unlocked.In this article, we’ll discuss the concept of textual data and how you can use it to extract valuable insights for your customer support operations.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "z3tze_AcrpeX",
        "outputId": "3e83ea7b-8211-4da5-b14d-bfea07f157d3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text =[]\n",
        "\n",
        "for i in text:\n",
        "  if i not in string.punctuation:\n",
        "    clean_text.append(i)\n",
        "\n",
        "proper_text = ''.join(clean_text)\n",
        "print(proper_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnazTlVJJ5yY",
        "outputId": "e7a4fc4f-2fc0-423a-cf19-8369b275071c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manually analyzing hundreds of unstructured textbased data sources is tedious and timeconsuming Whats worse is that since its in text format you know this process can be made easier but youre unsure how to do it If you’re in a datadriven company that relies on such data sources to make critical decisions to optimize and improve your customer experience processes keep readingAnd hes right Too often we get stuck in the DRIP syndrome where were happily generating data—but cant generate meaningful insights from them Also when you consider that 80 to 90 of data is unstructured  there’s so much potential waiting to be unlockedIn this article we’ll discuss the concept of textual data and how you can use it to extract valuable insights for your customer support operations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lower the text\n",
        "proper_text = proper_text.lower()\n",
        "\n",
        "print(proper_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XRuVJCmJ510",
        "outputId": "9aa91622-8a4e-40d4-ba71-15d48d754ecb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "manually analyzing hundreds of unstructured textbased data sources is tedious and timeconsuming whats worse is that since its in text format you know this process can be made easier but youre unsure how to do it if you’re in a datadriven company that relies on such data sources to make critical decisions to optimize and improve your customer experience processes keep readingand hes right too often we get stuck in the drip syndrome where were happily generating data—but cant generate meaningful insights from them also when you consider that 80 to 90 of data is unstructured  there’s so much potential waiting to be unlockedin this article we’ll discuss the concept of textual data and how you can use it to extract valuable insights for your customer support operations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# removing stopwords ****\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwbxNsCiJ53_",
        "outputId": "06fe3875-6dd4-423d-c1b0-e261a2587523"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = []\n",
        "\n",
        "for i in proper_text.split():\n",
        "  if i not in stopwords.words('english'):\n",
        "    cleaned_text.append(i)\n",
        "\n",
        "final_data = ' '.join(cleaned_text)\n",
        "print(final_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zoMmfiCJ56E",
        "outputId": "7adb0c62-8a37-4577-fe05-1ed56e009ccd"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "manually analyzing hundreds unstructured textbased data sources tedious timeconsuming whats worse since text format know process made easier youre unsure you’re datadriven company relies data sources make critical decisions optimize improve customer experience processes keep readingand hes right often get stuck drip syndrome happily generating data—but cant generate meaningful insights also consider 80 90 data unstructured there’s much potential waiting unlockedin article we’ll discuss concept textual data use extract valuable insights customer support operations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Stemming & Lemmatization\n",
        "#these 2 techniques are used to carry the  information how human brain  understands\n",
        "\n",
        "#Stemming on data\n",
        "from nltk.stem import PorterStemmer\n",
        "stemming = PorterStemmer()\n",
        "for i in final_data.split():\n",
        "  print(\"Before Stemming : \", i , \"------After stemming : \",stemming.stem(i))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-aEoRO0J58X",
        "outputId": "5481d09d-7941-4159-c43a-8f4a9442f17b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Stemming :  manually ------After stemming :  manual\n",
            "Before Stemming :  analyzing ------After stemming :  analyz\n",
            "Before Stemming :  hundreds ------After stemming :  hundr\n",
            "Before Stemming :  unstructured ------After stemming :  unstructur\n",
            "Before Stemming :  textbased ------After stemming :  textbas\n",
            "Before Stemming :  data ------After stemming :  data\n",
            "Before Stemming :  sources ------After stemming :  sourc\n",
            "Before Stemming :  tedious ------After stemming :  tediou\n",
            "Before Stemming :  timeconsuming ------After stemming :  timeconsum\n",
            "Before Stemming :  whats ------After stemming :  what\n",
            "Before Stemming :  worse ------After stemming :  wors\n",
            "Before Stemming :  since ------After stemming :  sinc\n",
            "Before Stemming :  text ------After stemming :  text\n",
            "Before Stemming :  format ------After stemming :  format\n",
            "Before Stemming :  know ------After stemming :  know\n",
            "Before Stemming :  process ------After stemming :  process\n",
            "Before Stemming :  made ------After stemming :  made\n",
            "Before Stemming :  easier ------After stemming :  easier\n",
            "Before Stemming :  youre ------After stemming :  your\n",
            "Before Stemming :  unsure ------After stemming :  unsur\n",
            "Before Stemming :  you’re ------After stemming :  you’r\n",
            "Before Stemming :  datadriven ------After stemming :  datadriven\n",
            "Before Stemming :  company ------After stemming :  compani\n",
            "Before Stemming :  relies ------After stemming :  reli\n",
            "Before Stemming :  data ------After stemming :  data\n",
            "Before Stemming :  sources ------After stemming :  sourc\n",
            "Before Stemming :  make ------After stemming :  make\n",
            "Before Stemming :  critical ------After stemming :  critic\n",
            "Before Stemming :  decisions ------After stemming :  decis\n",
            "Before Stemming :  optimize ------After stemming :  optim\n",
            "Before Stemming :  improve ------After stemming :  improv\n",
            "Before Stemming :  customer ------After stemming :  custom\n",
            "Before Stemming :  experience ------After stemming :  experi\n",
            "Before Stemming :  processes ------After stemming :  process\n",
            "Before Stemming :  keep ------After stemming :  keep\n",
            "Before Stemming :  readingand ------After stemming :  readingand\n",
            "Before Stemming :  hes ------After stemming :  he\n",
            "Before Stemming :  right ------After stemming :  right\n",
            "Before Stemming :  often ------After stemming :  often\n",
            "Before Stemming :  get ------After stemming :  get\n",
            "Before Stemming :  stuck ------After stemming :  stuck\n",
            "Before Stemming :  drip ------After stemming :  drip\n",
            "Before Stemming :  syndrome ------After stemming :  syndrom\n",
            "Before Stemming :  happily ------After stemming :  happili\n",
            "Before Stemming :  generating ------After stemming :  gener\n",
            "Before Stemming :  data—but ------After stemming :  data—but\n",
            "Before Stemming :  cant ------After stemming :  cant\n",
            "Before Stemming :  generate ------After stemming :  gener\n",
            "Before Stemming :  meaningful ------After stemming :  meaning\n",
            "Before Stemming :  insights ------After stemming :  insight\n",
            "Before Stemming :  also ------After stemming :  also\n",
            "Before Stemming :  consider ------After stemming :  consid\n",
            "Before Stemming :  80 ------After stemming :  80\n",
            "Before Stemming :  90 ------After stemming :  90\n",
            "Before Stemming :  data ------After stemming :  data\n",
            "Before Stemming :  unstructured ------After stemming :  unstructur\n",
            "Before Stemming :  there’s ------After stemming :  there’\n",
            "Before Stemming :  much ------After stemming :  much\n",
            "Before Stemming :  potential ------After stemming :  potenti\n",
            "Before Stemming :  waiting ------After stemming :  wait\n",
            "Before Stemming :  unlockedin ------After stemming :  unlockedin\n",
            "Before Stemming :  article ------After stemming :  articl\n",
            "Before Stemming :  we’ll ------After stemming :  we’ll\n",
            "Before Stemming :  discuss ------After stemming :  discuss\n",
            "Before Stemming :  concept ------After stemming :  concept\n",
            "Before Stemming :  textual ------After stemming :  textual\n",
            "Before Stemming :  data ------After stemming :  data\n",
            "Before Stemming :  use ------After stemming :  use\n",
            "Before Stemming :  extract ------After stemming :  extract\n",
            "Before Stemming :  valuable ------After stemming :  valuabl\n",
            "Before Stemming :  insights ------After stemming :  insight\n",
            "Before Stemming :  customer ------After stemming :  custom\n",
            "Before Stemming :  support ------After stemming :  support\n",
            "Before Stemming :  operations ------After stemming :  oper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d25twOqiJ5_w",
        "outputId": "57e80ef4-abfb-49ac-c7c1-26e0b53bae90"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatization = WordNetLemmatizer()\n",
        "for i in final_data.split():\n",
        "  print(\"Before Lemmatization : \",i , \"-------After Lemmaization : \", lemmatization.lemmatize(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRGgb7_bC52L",
        "outputId": "e7c304e2-de28-4af9-f3d7-85920333b68b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Lemmatization :  manually -------After Lemmaization :  manually\n",
            "Before Lemmatization :  analyzing -------After Lemmaization :  analyzing\n",
            "Before Lemmatization :  hundreds -------After Lemmaization :  hundred\n",
            "Before Lemmatization :  unstructured -------After Lemmaization :  unstructured\n",
            "Before Lemmatization :  textbased -------After Lemmaization :  textbased\n",
            "Before Lemmatization :  data -------After Lemmaization :  data\n",
            "Before Lemmatization :  sources -------After Lemmaization :  source\n",
            "Before Lemmatization :  tedious -------After Lemmaization :  tedious\n",
            "Before Lemmatization :  timeconsuming -------After Lemmaization :  timeconsuming\n",
            "Before Lemmatization :  whats -------After Lemmaization :  whats\n",
            "Before Lemmatization :  worse -------After Lemmaization :  worse\n",
            "Before Lemmatization :  since -------After Lemmaization :  since\n",
            "Before Lemmatization :  text -------After Lemmaization :  text\n",
            "Before Lemmatization :  format -------After Lemmaization :  format\n",
            "Before Lemmatization :  know -------After Lemmaization :  know\n",
            "Before Lemmatization :  process -------After Lemmaization :  process\n",
            "Before Lemmatization :  made -------After Lemmaization :  made\n",
            "Before Lemmatization :  easier -------After Lemmaization :  easier\n",
            "Before Lemmatization :  youre -------After Lemmaization :  youre\n",
            "Before Lemmatization :  unsure -------After Lemmaization :  unsure\n",
            "Before Lemmatization :  you’re -------After Lemmaization :  you’re\n",
            "Before Lemmatization :  datadriven -------After Lemmaization :  datadriven\n",
            "Before Lemmatization :  company -------After Lemmaization :  company\n",
            "Before Lemmatization :  relies -------After Lemmaization :  relies\n",
            "Before Lemmatization :  data -------After Lemmaization :  data\n",
            "Before Lemmatization :  sources -------After Lemmaization :  source\n",
            "Before Lemmatization :  make -------After Lemmaization :  make\n",
            "Before Lemmatization :  critical -------After Lemmaization :  critical\n",
            "Before Lemmatization :  decisions -------After Lemmaization :  decision\n",
            "Before Lemmatization :  optimize -------After Lemmaization :  optimize\n",
            "Before Lemmatization :  improve -------After Lemmaization :  improve\n",
            "Before Lemmatization :  customer -------After Lemmaization :  customer\n",
            "Before Lemmatization :  experience -------After Lemmaization :  experience\n",
            "Before Lemmatization :  processes -------After Lemmaization :  process\n",
            "Before Lemmatization :  keep -------After Lemmaization :  keep\n",
            "Before Lemmatization :  readingand -------After Lemmaization :  readingand\n",
            "Before Lemmatization :  hes -------After Lemmaization :  he\n",
            "Before Lemmatization :  right -------After Lemmaization :  right\n",
            "Before Lemmatization :  often -------After Lemmaization :  often\n",
            "Before Lemmatization :  get -------After Lemmaization :  get\n",
            "Before Lemmatization :  stuck -------After Lemmaization :  stuck\n",
            "Before Lemmatization :  drip -------After Lemmaization :  drip\n",
            "Before Lemmatization :  syndrome -------After Lemmaization :  syndrome\n",
            "Before Lemmatization :  happily -------After Lemmaization :  happily\n",
            "Before Lemmatization :  generating -------After Lemmaization :  generating\n",
            "Before Lemmatization :  data—but -------After Lemmaization :  data—but\n",
            "Before Lemmatization :  cant -------After Lemmaization :  cant\n",
            "Before Lemmatization :  generate -------After Lemmaization :  generate\n",
            "Before Lemmatization :  meaningful -------After Lemmaization :  meaningful\n",
            "Before Lemmatization :  insights -------After Lemmaization :  insight\n",
            "Before Lemmatization :  also -------After Lemmaization :  also\n",
            "Before Lemmatization :  consider -------After Lemmaization :  consider\n",
            "Before Lemmatization :  80 -------After Lemmaization :  80\n",
            "Before Lemmatization :  90 -------After Lemmaization :  90\n",
            "Before Lemmatization :  data -------After Lemmaization :  data\n",
            "Before Lemmatization :  unstructured -------After Lemmaization :  unstructured\n",
            "Before Lemmatization :  there’s -------After Lemmaization :  there’s\n",
            "Before Lemmatization :  much -------After Lemmaization :  much\n",
            "Before Lemmatization :  potential -------After Lemmaization :  potential\n",
            "Before Lemmatization :  waiting -------After Lemmaization :  waiting\n",
            "Before Lemmatization :  unlockedin -------After Lemmaization :  unlockedin\n",
            "Before Lemmatization :  article -------After Lemmaization :  article\n",
            "Before Lemmatization :  we’ll -------After Lemmaization :  we’ll\n",
            "Before Lemmatization :  discuss -------After Lemmaization :  discus\n",
            "Before Lemmatization :  concept -------After Lemmaization :  concept\n",
            "Before Lemmatization :  textual -------After Lemmaization :  textual\n",
            "Before Lemmatization :  data -------After Lemmaization :  data\n",
            "Before Lemmatization :  use -------After Lemmaization :  use\n",
            "Before Lemmatization :  extract -------After Lemmaization :  extract\n",
            "Before Lemmatization :  valuable -------After Lemmaization :  valuable\n",
            "Before Lemmatization :  insights -------After Lemmaization :  insight\n",
            "Before Lemmatization :  customer -------After Lemmaization :  customer\n",
            "Before Lemmatization :  support -------After Lemmaization :  support\n",
            "Before Lemmatization :  operations -------After Lemmaization :  operation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2RxRzZMlD8Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatized_text = []\n",
        "lemmatization = WordNetLemmatizer()\n",
        "for i in final_data.split():\n",
        "  lemmatized_text.append(lemmatization.lemmatize(i))\n",
        "lemmatized_text = ' '.join(lemmatized_text)\n",
        "print(lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_6wAPudC55I",
        "outputId": "390800d9-64a0-43c0-ca13-811028a56cec"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "manually analyzing hundred unstructured textbased data source tedious timeconsuming whats worse since text format know process made easier youre unsure you’re datadriven company relies data source make critical decision optimize improve customer experience process keep readingand he right often get stuck drip syndrome happily generating data—but cant generate meaningful insight also consider 80 90 data unstructured there’s much potential waiting unlockedin article we’ll discus concept textual data use extract valuable insight customer support operation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for converting text to numbers we can use Bag of Words , TF-IDF[Term frequency and Inverse Document Frequency] and Embeded techniques"
      ],
      "metadata": {
        "id": "txym6BtlC58C"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag of words:\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # for bag of words\n",
        "\n",
        "res = CountVectorizer()\n",
        "\n",
        "values = res.fit_transform([lemmatized_text])\n",
        "print(values.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbLNLxKwC5-8",
        "outputId": "66fa8573-6c57-43dd-d79c-4fbc535ff1fd"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 1 1 1 1 1 1 1 1 2 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## To make some difference between words we are gpoing to use TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "res = TfidfVectorizer()\n",
        "\n",
        "values = res.fit_transform([lemmatized_text])\n",
        "\n",
        "print(values.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Fqpmlr7C6Bo",
        "outputId": "9c64e9b8-e392-4cb6-f103-d81053031d81"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.09667365 0.09667365 0.09667365 0.09667365 0.09667365 0.09667365\n",
            "  0.09667365 0.09667365 0.09667365 0.09667365 0.09667365 0.1933473\n",
            "  0.48336824 0.09667365 0.09667365 0.09667365 0.09667365 0.09667365\n",
            "  0.09667365 0.09667365 0.09667365 0.09667365 0.09667365 0.09667365\n",
            "  0.09667365 0.09667365 0.09667365 0.09667365 0.1933473  0.09667365\n",
            "  0.09667365 0.09667365 0.09667365 0.09667365 0.09667365 0.09667365\n",
            "  0.09667365 0.09667365 0.09667365 0.09667365 0.09667365 0.1933473\n",
            "  0.09667365 0.09667365 0.09667365 0.09667365 0.09667365 0.1933473\n",
            "  0.09667365 0.09667365 0.09667365 0.09667365 0.09667365 0.09667365\n",
            "  0.09667365 0.09667365 0.09667365 0.09667365 0.1933473  0.09667365\n",
            "  0.09667365 0.09667365 0.09667365 0.09667365 0.09667365 0.09667365\n",
            "  0.09667365 0.09667365]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Embedding Technique(*****):\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n"
      ],
      "metadata": {
        "id": "DUs_CxL1C6EO"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#integer encode the documents\n",
        "vocab_size = 100\n",
        "encoded_doc = [one_hot(d, vocab_size)for d in lemmatized_text]\n",
        "print(encoded_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiZhv3W2C6HH",
        "outputId": "369fcf21-97bf-479b-cdcf-f0c92f03f83f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[11], [84], [85], [91], [84], [59], [59], [98], [], [84], [85], [84], [59], [98], [55], [24], [85], [52], [], [32], [91], [85], [58], [56], [21], [58], [], [91], [85], [39], [57], [56], [91], [59], [57], [91], [56], [21], [58], [], [57], [21], [55], [57], [13], [84], [39], [21], [58], [], [58], [84], [57], [84], [], [39], [21], [91], [56], [59], [21], [], [57], [21], [58], [24], [21], [91], [39], [], [57], [24], [11], [21], [59], [21], [85], [39], [91], [11], [24], [85], [52], [], [60], [32], [84], [57], [39], [], [60], [21], [56], [39], [21], [], [39], [24], [85], [59], [21], [], [57], [21], [55], [57], [], [5], [21], [56], [11], [84], [57], [], [92], [85], [21], [60], [], [40], [56], [21], [59], [21], [39], [39], [], [11], [84], [58], [21], [], [21], [84], [39], [24], [21], [56], [], [98], [21], [91], [56], [21], [], [91], [85], [39], [91], [56], [21], [], [98], [21], [91], [61], [56], [21], [], [58], [84], [57], [84], [58], [56], [24], [34], [21], [85], [], [59], [21], [11], [40], [84], [85], [98], [], [56], [21], [59], [24], [21], [39], [], [58], [84], [57], [84], [], [39], [21], [91], [56], [59], [21], [], [11], [84], [92], [21], [], [59], [56], [24], [57], [24], [59], [84], [59], [], [58], [21], [59], [24], [39], [24], [21], [85], [], [21], [40], [57], [24], [11], [24], [55], [21], [], [24], [11], [40], [56], [21], [34], [21], [], [59], [91], [39], [57], [21], [11], [21], [56], [], [21], [55], [40], [21], [56], [24], [21], [85], [59], [21], [], [40], [56], [21], [59], [21], [39], [39], [], [92], [21], [21], [40], [], [56], [21], [84], [58], [24], [85], [52], [84], [85], [58], [], [32], [21], [], [56], [24], [52], [32], [57], [], [21], [5], [57], [21], [85], [], [52], [21], [57], [], [39], [57], [91], [59], [92], [], [58], [56], [24], [40], [], [39], [98], [85], [58], [56], [21], [11], [21], [], [32], [84], [40], [40], [24], [59], [98], [], [52], [21], [85], [21], [56], [84], [57], [24], [85], [52], [], [58], [84], [57], [84], [39], [13], [91], [57], [], [59], [84], [85], [57], [], [52], [21], [85], [21], [56], [84], [57], [21], [], [11], [21], [84], [85], [24], [85], [52], [5], [91], [59], [], [24], [85], [39], [24], [52], [32], [57], [], [84], [59], [39], [21], [], [59], [21], [85], [39], [24], [58], [21], [56], [], [11], [23], [], [17], [23], [], [58], [84], [57], [84], [], [91], [85], [39], [57], [56], [91], [59], [57], [91], [56], [21], [58], [], [57], [32], [21], [56], [21], [61], [39], [], [11], [91], [59], [32], [], [40], [21], [57], [21], [85], [57], [24], [84], [59], [], [60], [84], [24], [57], [24], [85], [52], [], [91], [85], [59], [21], [59], [92], [21], [58], [24], [85], [], [84], [56], [57], [24], [59], [59], [21], [], [60], [21], [61], [59], [59], [], [58], [24], [39], [59], [91], [39], [], [59], [21], [85], [59], [21], [40], [57], [], [57], [21], [55], [57], [91], [84], [59], [], [58], [84], [57], [84], [], [91], [39], [21], [], [21], [55], [57], [56], [84], [59], [57], [], [34], [84], [59], [91], [84], [13], [59], [21], [], [24], [85], [39], [24], [52], [32], [57], [], [59], [91], [39], [57], [21], [11], [21], [56], [], [39], [91], [40], [40], [21], [56], [57], [], [21], [40], [21], [56], [84], [57], [24], [21], [85]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(encoded_doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtsDcbCBvClN",
        "outputId": "3d03df6a-5ab9-4d5e-add1-758744c7f0b0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove null values from encoded_doc\n",
        "\n",
        "encoded_doc = [d for d in encoded_doc if d != [] * vocab_size]\n"
      ],
      "metadata": {
        "id": "njc0C3wDC6Mh"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oai7eUY0C6PO",
        "outputId": "1e5bc874-eddc-4b19-cb4c-58a608d52a16"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[11], [84], [85], [91], [84], [59], [59], [98], [84], [85], [84], [59], [98], [55], [24], [85], [52], [32], [91], [85], [58], [56], [21], [58], [91], [85], [39], [57], [56], [91], [59], [57], [91], [56], [21], [58], [57], [21], [55], [57], [13], [84], [39], [21], [58], [58], [84], [57], [84], [39], [21], [91], [56], [59], [21], [57], [21], [58], [24], [21], [91], [39], [57], [24], [11], [21], [59], [21], [85], [39], [91], [11], [24], [85], [52], [60], [32], [84], [57], [39], [60], [21], [56], [39], [21], [39], [24], [85], [59], [21], [57], [21], [55], [57], [5], [21], [56], [11], [84], [57], [92], [85], [21], [60], [40], [56], [21], [59], [21], [39], [39], [11], [84], [58], [21], [21], [84], [39], [24], [21], [56], [98], [21], [91], [56], [21], [91], [85], [39], [91], [56], [21], [98], [21], [91], [61], [56], [21], [58], [84], [57], [84], [58], [56], [24], [34], [21], [85], [59], [21], [11], [40], [84], [85], [98], [56], [21], [59], [24], [21], [39], [58], [84], [57], [84], [39], [21], [91], [56], [59], [21], [11], [84], [92], [21], [59], [56], [24], [57], [24], [59], [84], [59], [58], [21], [59], [24], [39], [24], [21], [85], [21], [40], [57], [24], [11], [24], [55], [21], [24], [11], [40], [56], [21], [34], [21], [59], [91], [39], [57], [21], [11], [21], [56], [21], [55], [40], [21], [56], [24], [21], [85], [59], [21], [40], [56], [21], [59], [21], [39], [39], [92], [21], [21], [40], [56], [21], [84], [58], [24], [85], [52], [84], [85], [58], [32], [21], [56], [24], [52], [32], [57], [21], [5], [57], [21], [85], [52], [21], [57], [39], [57], [91], [59], [92], [58], [56], [24], [40], [39], [98], [85], [58], [56], [21], [11], [21], [32], [84], [40], [40], [24], [59], [98], [52], [21], [85], [21], [56], [84], [57], [24], [85], [52], [58], [84], [57], [84], [39], [13], [91], [57], [59], [84], [85], [57], [52], [21], [85], [21], [56], [84], [57], [21], [11], [21], [84], [85], [24], [85], [52], [5], [91], [59], [24], [85], [39], [24], [52], [32], [57], [84], [59], [39], [21], [59], [21], [85], [39], [24], [58], [21], [56], [11], [23], [17], [23], [58], [84], [57], [84], [91], [85], [39], [57], [56], [91], [59], [57], [91], [56], [21], [58], [57], [32], [21], [56], [21], [61], [39], [11], [91], [59], [32], [40], [21], [57], [21], [85], [57], [24], [84], [59], [60], [84], [24], [57], [24], [85], [52], [91], [85], [59], [21], [59], [92], [21], [58], [24], [85], [84], [56], [57], [24], [59], [59], [21], [60], [21], [61], [59], [59], [58], [24], [39], [59], [91], [39], [59], [21], [85], [59], [21], [40], [57], [57], [21], [55], [57], [91], [84], [59], [58], [84], [57], [84], [91], [39], [21], [21], [55], [57], [56], [84], [59], [57], [34], [84], [59], [91], [84], [13], [59], [21], [24], [85], [39], [24], [52], [32], [57], [59], [91], [39], [57], [21], [11], [21], [56], [39], [91], [40], [40], [21], [56], [57], [21], [40], [21], [56], [84], [57], [24], [21], [85]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(encoded_doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4-NWfBJtkoY",
        "outputId": "ea0e1e55-d404-47a3-ca74-93d33945fb0c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#learn with example\n",
        "\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "'Great effort',\n",
        "'nice work',\n",
        "'Excellent!',\n",
        "'Weak',\n",
        "'Poor effort!',\n",
        "'not good',\n",
        "'poor work',\n",
        "'Could have done better.']\n",
        "\n",
        "labels = [1,1,1,1,1,0,0,0,0,0] # defining class labels"
      ],
      "metadata": {
        "id": "cvwZyHeAC6R8"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# integer encode the documents\n",
        "vocab_size = 100\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tk-Z2jaw0nY",
        "outputId": "7fba5710-3eb7-4055-86c9-43d022fe0a71"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[57, 7], [19, 14], [49, 8], [27, 14], [81], [3], [28, 8], [62, 19], [28, 14], [1, 50, 7, 32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yq57t6Gw0rH",
        "outputId": "cd9926ef-abd6-416e-8e07-584e947a8e1d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[57  7  0  0]\n",
            " [19 14  0  0]\n",
            " [49  8  0  0]\n",
            " [27 14  0  0]\n",
            " [81  0  0  0]\n",
            " [ 3  0  0  0]\n",
            " [28  8  0  0]\n",
            " [62 19  0  0]\n",
            " [28 14  0  0]\n",
            " [ 1 50  7 32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "metadata": {
        "id": "wV8Fngksw0uL"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "NVByLzb6w0wW"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(layers.Embedding(vocab_size,8,input_length = max_length))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1,activation = 'sigmoid'))"
      ],
      "metadata": {
        "id": "qbDfgVqyiMlj"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "labels=np.array(labels)"
      ],
      "metadata": {
        "id": "ob310O_fiMpv"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# summarize the model\n",
        "model.summary()\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=100, verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwJ9tMJ6wRpv",
        "outputId": "67bc9929-1fef-4a66-a86b-2b3643f2f6d4"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 4, 8)              800       \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 833 (3.25 KB)\n",
            "Trainable params: 833 (3.25 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "1/1 - 1s - loss: 0.6914 - acc: 0.6000 - 702ms/epoch - 702ms/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 0.6900 - acc: 0.6000 - 9ms/epoch - 9ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.6885 - acc: 0.6000 - 9ms/epoch - 9ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.6871 - acc: 0.5000 - 8ms/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.6857 - acc: 0.6000 - 9ms/epoch - 9ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.6843 - acc: 0.7000 - 8ms/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 0.6829 - acc: 0.8000 - 8ms/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 0.6815 - acc: 0.8000 - 8ms/epoch - 8ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 0.6801 - acc: 0.8000 - 8ms/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 0.6787 - acc: 0.8000 - 8ms/epoch - 8ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 0.6773 - acc: 0.8000 - 7ms/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 0.6759 - acc: 0.8000 - 8ms/epoch - 8ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 0.6745 - acc: 0.8000 - 8ms/epoch - 8ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 0.6731 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 0.6717 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 0.6703 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 0.6689 - acc: 0.9000 - 13ms/epoch - 13ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 0.6675 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 0.6660 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 0.6646 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 0.6632 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 0.6617 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 0.6603 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 0.6588 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 0.6573 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 0.6558 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 0.6543 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 0.6528 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 0.6513 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 0.6497 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 0.6482 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 0.6466 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 0.6450 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 0.6434 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 0.6418 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 0.6402 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 0.6386 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 0.6370 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 0.6353 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 0.6336 - acc: 0.9000 - 10ms/epoch - 10ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 0.6320 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 0.6303 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 0.6286 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 0.6269 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 0.6252 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 0.6234 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 0.6217 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 0.6199 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 0.6181 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 0.6164 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 0.6146 - acc: 0.9000 - 11ms/epoch - 11ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 0.6128 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 0.6109 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 0.6091 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 0.6073 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 0.6054 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 0.6036 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 0.6017 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 0.5998 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 0.5979 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 0.5960 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 0.5941 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 0.5922 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 0.5902 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 0.5883 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 0.5863 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 0.5843 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 0.5824 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 0.5804 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 0.5784 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 0.5764 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 0.5744 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 0.5724 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 0.5703 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 0.5683 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 0.5663 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 0.5642 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 0.5622 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 0.5601 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 0.5580 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 0.5559 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 0.5539 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 0.5518 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 0.5497 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 0.5476 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 0.5455 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 0.5433 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 0.5412 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 0.5391 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 0.5370 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 0.5348 - acc: 0.9000 - 17ms/epoch - 17ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 0.5327 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 0.5305 - acc: 0.9000 - 7ms/epoch - 7ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 0.5284 - acc: 0.9000 - 9ms/epoch - 9ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 0.5262 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 0.5241 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 0.5219 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 0.5198 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 0.5176 - acc: 0.9000 - 8ms/epoch - 8ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 0.5154 - acc: 0.9000 - 7ms/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d3e041b2950>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0NXuYax2Ktw",
        "outputId": "23102ce2-987f-499b-e158-d6586771c713"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 89.999998\n"
          ]
        }
      ]
    }
  ]
}